{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement for Students**\n",
        "You're tasked with analyzing the text of Alice in Wonderland to understand the structure and meaning of its words using natural language processing (NLP) and data visualization techniques.\n",
        "\n",
        "## Goals:\n",
        "- Clean and preprocess the text\n",
        "\n",
        "- Visualize word frequencies\n",
        "\n",
        "- Word cloud\n",
        "\n",
        "- Bar chart\n",
        "\n",
        "- Plot semantic relationships\n",
        "\n",
        "- Use GloVe embeddings + PCA\n",
        "\n",
        "- Display word similarities\n",
        "\n",
        "- Heatmap"
      ],
      "metadata": {
        "id": "Vw2j99TAGrBC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intall Library"
      ],
      "metadata": {
        "id": "ik73J1KBRkcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim -q\n"
      ],
      "metadata": {
        "id": "b4fdKvaaRiQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scipy --upgrade -q"
      ],
      "metadata": {
        "id": "Qe3zihXFhpak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Library"
      ],
      "metadata": {
        "id": "dlMgVvAgHKJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import seaborn as sns\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "import gensim.downloader as api"
      ],
      "metadata": {
        "id": "869Y1kKdHP6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and Clean the Text"
      ],
      "metadata": {
        "id": "Iv2pVQovHZ_O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oc-EuejnGlrK"
      },
      "outputs": [],
      "source": [
        "# load the text\n",
        "url = \"https://www.gutenberg.org/files/11/11-0.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean text\n",
        "text = text.lower()\n",
        "text = re.sub(r'[^a-z\\s]', '', text)\n",
        "words = text.split()\n",
        "print(f\"total words: {len(words)}\")"
      ],
      "metadata": {
        "id": "pSD11E0iHvs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text[:500]"
      ],
      "metadata": {
        "id": "aHBUGpJFKNOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "m1Nh2QkQIJS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get tokenization"
      ],
      "metadata": {
        "id": "Ddd4qnmaJkGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK's Punkt tokenizer model\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Break down the text into lexical tokens (words and punctuation)\n",
        "word_tokens  = word_tokenize(text)"
      ],
      "metadata": {
        "id": "2LnK26G_JmaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'{word_tokens} \\n')\n",
        "print(f'total tokens: {len(word_tokens)}')"
      ],
      "metadata": {
        "id": "m49sCiVENAn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## stopwords"
      ],
      "metadata": {
        "id": "sgkufr-rJaGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get list stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Get stopwords from NLTK\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Create custom stopwords to remove more common abbreviations\n",
        "custom_stopwords = {\n",
        "    \"arent\", \"cant\", \"couldnt\", \"didnt\", \"doesnt\", \"dont\", \"hadnt\", \"hasnt\", \"havent\",\n",
        "    \"hed\", \"hell\", \"hes\", \"id\", \"ill\", \"im\", \"ive\",\n",
        "    \"isnt\", \"lets\", \"mightnt\", \"mustnt\", \"shant\",\n",
        "    \"shed\", \"shell\", \"shes\",\n",
        "    \"shouldnt\", \"thats\", \"theres\", \"theyd\", \"theyll\", \"theyre\", \"theyve\",\n",
        "    \"wed\", \"were\", \"weve\", \"werent\",\n",
        "    \"whatll\", \"whatre\", \"whats\", \"whatve\",\n",
        "    \"wheres\", \"whod\", \"wholl\", \"whore\", \"whos\", \"whove\",\n",
        "    \"wont\", \"wouldnt\", \"youd\", \"youll\", \"youre\", \"youve\"\n",
        "}\n",
        "\n",
        "# update stopwords\n",
        "stop_words.update(custom_stopwords)\n",
        "print(f'stopwords: {stop_words} \\n')\n"
      ],
      "metadata": {
        "id": "bzm3Bww9Iplm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove stopdwords\n",
        "clean_tokens  = [word for word in word_tokens if word not in stop_words and len(word) > 2]\n",
        "print(f\"clean tokens: {clean_tokens} \\n\")\n",
        "print(f'total tokens:{len(clean_tokens)}')"
      ],
      "metadata": {
        "id": "juQnmnK4ILit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "A5ckQrTKQM5E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## word cloud"
      ],
      "metadata": {
        "id": "cMoACvLYRAYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_freq = Counter(clean_tokens)\n",
        "\n",
        "# Word Cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Word Cloud of Alice in Wonderland\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rg672VGTQW3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## bar chart"
      ],
      "metadata": {
        "id": "DH8-G_eGRD04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bar Chart\n",
        "most_common = word_freq.most_common(20)\n",
        "words_, freqs = zip(*most_common)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(words_, freqs)\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Top 20 Most Frequent Words\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VaAb5CQuQmwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GloVe Embeddings"
      ],
      "metadata": {
        "id": "QMis5AH8Q4sD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load GloVe embeddings (50-d)\n",
        "glove_vectors = api.load(\"glove-wiki-gigaword-50\")"
      ],
      "metadata": {
        "id": "se-ozJUkS0Be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a list of unique tokens (remove duplicates from the cleaned tokens)\n",
        "unique_tokens = list(set(clean_tokens))\n",
        "print(f'Total unique tokens: {len(unique_tokens)}\\n')\n",
        "\n",
        "# Filter out tokens that exist in the GloVe vocabulary\n",
        "filtered_words = [word for word in unique_tokens if word in glove_vectors]\n",
        "print(f'Filtered words (found in GloVe): {filtered_words}\\n')\n",
        "print(f'total filtered words: {len(filtered_words)} \\n')\n",
        "\n",
        "# Create a NumPy array of word embeddings (vectors) for the filtered words\n",
        "# Each word is represented as a numerical vector (e.g., 50-dimensional if using GloVe-50D)\n",
        "\n",
        "embeddings = np.array([glove_vectors[word] for word in filtered_words])\n",
        "print(f'Embeddings array (vector representation of words):\\n{embeddings}')\n"
      ],
      "metadata": {
        "id": "yMHVb3w0RNw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part-of-Speech Tagging"
      ],
      "metadata": {
        "id": "1hos_omqXnj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the tagger resource (ENG)\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "# Tag POS for each word\n",
        "pos_tags = nltk.pos_tag(filtered_words)\n",
        "print(f'Tag POS: {pos_tags} \\n')"
      ],
      "metadata": {
        "id": "t0ipAFsuWRj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simplify POS tag\n",
        "def simplify_pos(tag):\n",
        "    if tag.startswith('NN'):\n",
        "        return 'noun'\n",
        "    elif tag.startswith('VB'):\n",
        "        return 'verb'\n",
        "    elif tag.startswith('JJ'):\n",
        "        return 'adjective'\n",
        "    elif tag.startswith('RB'):\n",
        "        return 'adverb'\n",
        "    else:\n",
        "        return 'other'\n",
        "\n",
        "word_pos = {word: simplify_pos(tag) for word, tag in pos_tags}\n",
        "print(f'word_pos: {word_pos} \\n')"
      ],
      "metadata": {
        "id": "5SfNJBbhWzsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "color_map = {\n",
        "    'noun': 'red',\n",
        "    'verb': 'blue',\n",
        "    'adjective': 'green',\n",
        "    'adverb': 'purple',\n",
        "    'other': 'gray'\n",
        "}"
      ],
      "metadata": {
        "id": "JRVN-tzfZobv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA"
      ],
      "metadata": {
        "id": "tFNoF9t0XwVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA to 2D\n",
        "pca = PCA(n_components=2)\n",
        "reduced = pca.fit_transform(embeddings)"
      ],
      "metadata": {
        "id": "Ssw7otwFUmX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word frequency\n",
        "word_freq = Counter(clean_tokens)\n",
        "print(f'word freq: {word_freq}')"
      ],
      "metadata": {
        "id": "bUp5yTLeTCz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for Plotly\n",
        "x_vals = reduced[:, 0]\n",
        "y_vals = reduced[:, 1]\n",
        "colors = [color_map.get(word_pos[word], 'black') for word in filtered_words]\n",
        "sizes = [min(word_freq[word] * 3, 40) for word in filtered_words]\n",
        "hover_texts = [f\"{word}<br>POS: {word_pos[word]}<br>Freq: {word_freq[word]}\" for word in filtered_words]"
      ],
      "metadata": {
        "id": "nZCotffKbDUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create scatter plot\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add traces for each POS type to show legend color\n",
        "for pos_type, color in color_map.items():\n",
        "    # Get all indices of words for this POS type\n",
        "    indices = [i for i, word in enumerate(filtered_words) if word_pos[word] == pos_type]\n",
        "    if indices:\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=[x_vals[i] for i in indices],\n",
        "            y=[y_vals[i] for i in indices],\n",
        "            mode='markers',\n",
        "            name=pos_type.capitalize(),\n",
        "            marker=dict(\n",
        "                size=[sizes[i] for i in indices],\n",
        "                color=color,\n",
        "                opacity=0.7,\n",
        "                line=dict(width=0.5, color='black')\n",
        "            ),\n",
        "            hovertext=[hover_texts[i] for i in indices],\n",
        "            hoverinfo='text',\n",
        "            showlegend=True\n",
        "        ))\n",
        "\n",
        "# Customize layout with legend on the left\n",
        "fig.update_layout(\n",
        "    title='PCA of Word Embeddings<br><sup>Color = POS Type | Size = Word Frequency</sup>',\n",
        "    xaxis_title='PC1',\n",
        "    yaxis_title='PC2',\n",
        "    template='plotly_white',\n",
        "    width=950,\n",
        "    height=700,\n",
        "    legend=dict(\n",
        "        title='Part of Speech',\n",
        "        x=0.01,\n",
        "        xanchor='left',\n",
        "        y=1,\n",
        "        yanchor='top',\n",
        "        bgcolor='rgba(255,255,255,0.8)',\n",
        "        bordercolor='black',\n",
        "        borderwidth=1\n",
        "    )\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "Sgcva8RhbGSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Heatmap"
      ],
      "metadata": {
        "id": "e-YB5OHsUv_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of top frequent words to include in similarity analysis\n",
        "top_words_count = 20\n",
        "\n",
        "# Extract top frequent words (expand selection to ensure valid GloVe coverage)\n",
        "frequent_words = [word for word, _ in word_freq.most_common(top_words_count * 3)]\n",
        "\n",
        "# Keep only words that are present in the GloVe vocabulary\n",
        "words_in_glove = [word for word in frequent_words if word in glove_vectors][:top_words_count]\n",
        "\n",
        "# Ensure there are enough valid words for plotting\n",
        "enough_words = len(words_in_glove) >= 2\n",
        "\n",
        "# If valid, continue to compute similarity and plot\n",
        "if not enough_words:\n",
        "    print(\"Not enough valid words found in GloVe to plot the heatmap\")\n",
        "else:\n",
        "    # Get GloVe vector embeddings for the selected words\n",
        "    word_embeddings = np.array([glove_vectors[word] for word in words_in_glove])\n",
        "\n",
        "    # Compute pairwise cosine similarity\n",
        "    similarity_matrix = cosine_similarity(word_embeddings)\n",
        "\n",
        "    # Plot heatmap of cosine similarity\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(\n",
        "        similarity_matrix,\n",
        "        xticklabels=words_in_glove,\n",
        "        yticklabels=words_in_glove,\n",
        "        cmap='coolwarm',\n",
        "        annot=True,\n",
        "        fmt=\".2f\"\n",
        "    )\n",
        "    plt.title(f\"Cosine Similarity Heatmap of Top {len(words_in_glove)} Frequent Words\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "dIyIrqtIePpJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}